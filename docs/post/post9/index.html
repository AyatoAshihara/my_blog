<!DOCTYPE html>
<html lang="en-us">
<head>  <link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/xcode.min.css" rel="stylesheet">

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<script type="application/ld+json">
    {
        "@context" : "http://schema.org",
        "@type" : "BlogPosting",
        "mainEntityOfPage": {
             "@type": "WebPage",
             "@id": "\/my_blog\/"
        },
        "articleSection" : "post",
        "name" : "【日次GDP】GPLVMでマルチファクターモデルを構築してみた",
        "headline" : "【日次GDP】GPLVMでマルチファクターモデルを構築してみた",
        "description" : "\r\n\x3cstyle type=\x22text\/css\x22\x3e\r\na.sourceLine { display: inline-block; line-height: 1.25; }\r\na.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\r\na.sourceLine:empty { height: 1.2em; }\r\n.sourceCode { overflow: visible; }\r\ncode.sourceCode { white-space: pre; position: relative; }\r\ndiv.sourceCode { margin: 1em 0; }\r\npre.sourceCode { margin: 0; }\r\n@media screen {\r\ndiv.sourceCode { overflow: auto; }\r\n}\r\n@media print {\r\ncode.sourceCode { white-space: pre-wrap; }\r\na.sourceLine { text-indent: -1em; padding-left: 1em; }\r\n}\r\npre.numberSource a.sourceLine\r\n  { position: relative; left: -4em; }\r\npre.numberSource a.sourceLine::before\r\n  { content: attr(title);\r\n    position: relative; left: -1em; text-align: right; vertical-align: baseline;\r\n    border: none; pointer-events: all; display: inline-block;\r\n    -webkit-touch-callout: none; -webkit-user-select: none;\r\n    -khtml-user-select: none; -moz-user-select: none;\r\n    -ms-user-select: none; user-select: none;\r\n    padding: 0 4px; width: 4em;\r\n    color: #aaaaaa;\r\n  }\r\npre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }\r\ndiv.sourceCode\r\n  { background-color: #f8f8f8; }\r\n@media screen {\r\na.sourceLine::before { text-decoration: underline; }\r\n}\r\ncode span.al { color: #ef2929; } \/* Alert *\/\r\ncode span.an { color: #8f5902; font-weight: bold; font-style: italic; } \/* Annotation *\/\r\ncode span.at { color: #c4a000; } \/* Attribute *\/\r\ncode span.bn { color: #0000cf; } \/* BaseN *\/\r\ncode span.cf { color: #204a87; font-weight: bold; } \/* ControlFlow *\/\r\ncode span.ch { color: #4e9a06; } \/* Char *\/\r\ncode span.cn { color: #000000; } \/* Constant *\/\r\ncode span.co { color: #8f5902; font-style: italic; } \/* Comment *\/\r\ncode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } \/* CommentVar *\/\r\ncode span.do { color: #8f5902; font-weight: bold; font-style: italic; } \/* Documentation *\/\r\ncode span.dt { color: #204a87; } \/* DataType *\/\r\ncode span.dv { color: #0000cf; } \/* DecVal *\/\r\ncode span.er { color: #a40000; font-weight: bold; } \/* Error *\/\r\ncode span.ex { } \/* Extension *\/\r\ncode span.fl { color: #0000cf; } \/* Float *\/\r\ncode span.fu { color: #000000; } \/* Function *\/\r\ncode span.im { } \/* Import *\/\r\ncode span.in { color: #8f5902; font-weight: bold; font-style: italic; } \/* Information *\/\r\ncode span.kw { color: #204a87; font-weight: bold; } \/* Keyword *\/\r\ncode span.op { color: #ce5c00; font-weight: bold; } \/* Operator *\/\r\ncode span.ot { color: #8f5902; } \/* Other *\/\r\ncode span.pp { color: #8f5902; font-style: italic; } \/* Preprocessor *\/\r\ncode span.sc { color: #000000; } \/* SpecialChar *\/\r\ncode span.ss { color: #4e9a06; } \/* SpecialString *\/\r\ncode span.st { color: #4e9a06; } \/* String *\/\r\ncode span.va { color: #000000; } \/* Variable *\/\r\ncode span.vs { color: #4e9a06; } \/* VerbatimString *\/\r\ncode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } \/* Warning *\/\r\n\x3c\/style\x3e\r\n\r\n\r\n\x3cp\x3e最近私が注目しているのがガウス過程。今回はそのガウス過程の中でも、主成分分析のようにデータセットの次元削減のために使用されるGaussian Process Latent Variable Model（GPLVM）の紹介をします。このモデルは主成分分析のド発展版で、確率的主成分分析のように尤度を評価できるので圧縮する最適な次元を決定でき、また非線形変換をかけるので複雑なデータ構造を持つデータセットでも主成分がきれいに分かれるのが特徴です。今回は毎度使用しているe-statの経済統計データでGPLVMを実装し、GDP予測モデルを作ってみました。Giannone et. al. (2008)の発展版です。\x3c\/p\x3e\r\n",
        "inLanguage" : "en",
        "author" : "Ayato Ashihara",
        "creator" : "Ayato Ashihara",
        "publisher": "Ayato Ashihara",
        "accountablePerson" : "Ayato Ashihara",
        "copyrightHolder" : "Ayato Ashihara",
        "copyrightYear" : "2019",
        "datePublished": "2019-05-26 00:00:00 \x2b0000 UTC",
        "dateModified" : "2019-05-26 00:00:00 \x2b0000 UTC",
        "url" : "\/my_blog\/post\/post9\/",
        "wordCount" : "1004",
        "image" : "/my_blog/%!s(\u003cnil\u003e)"",
        "keywords" : [ ""R"",""ガウス過程"","Blog" ]   
    }
    </script>


 <title>【日次GDP】GPLVMでマルチファクターモデルを構築してみた </title>


<meta name="description" content="マクロ経済、機械学習系の記事を投稿しています。" />



<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="robots" content="all,follow">
<meta name="googlebot" content="index,follow,snippet,archive">
<link rel="stylesheet" id="ct-tracks-google-fonts-css" href="//fonts.googleapis.com/css?family=Raleway%3A400%2C700&amp;subset=latin%2Clatin-ext&amp;ver=4.7.2" type="text/css" media="all">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

<link href="/my_blog/css/style.css?v=1561192814" rel="stylesheet" id="theme-stylesheet" type='text/css' media='all'>

<link href="/my_blog/css/custom.css?v=1561192814" rel="stylesheet" type='text/css' media='all'>
<link rel="shortcut icon" href="/my_blog/img/favicon.ico" type="image/x-icon">
<link rel="icon" href="/my_blog/img/favicon.ico" type="image/x-icon">


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-140804055-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

</head>


<body class="post-template-default single single-post single-format-standard ct-body singular singular-post not-front standard">

  <div id="overflow-container" class="overflow-container">
    <a class="skip-content" href="#main">Skip to content</a>
    <header id="site-header" class="site-header" role="banner">
      <div class='top-navigation'>
        <div class='container'>

  <div id="menu-secondary" class="menu-container menu-secondary" role="navigation">
    <button id="toggle-secondary-navigation" class="toggle-secondary-navigation"><i class="fas fa-plus"></i></button>

    <div class="menu">

      <ul id="menu-secondary-items" class="menu-secondary-items">
        
        <li id="menu-item" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item">
          <a href="/my_blog/categories/%E4%BB%95%E4%BA%8B%E9%96%A2%E9%80%A3">仕事関連</a>
        </li>
        
        <li id="menu-item" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item">
          <a href="/my_blog/categories/%E6%97%A5%E6%AC%A1gdp">日次gdp</a>
        </li>
        

      </ul>

    </div>

  </div>


  <ul class="social-media-icons">


    
    <li>
      <a href="https://www.facebook.com/ASSIY" data-animate-hover="pulse" class="facebook" target="_blank">
        <i class="fab fa-facebook-square" title="facebook"></i>
        <span class="screen-reader-text">facebook</span>
      </a>
    </li>
    

    

    

    

    
    <li>
      <a href="mailto:assiy119@yahoo.co.jp" data-animate-hover="pulse" class="email">
        <i class="fas fa-envelope" title="email"></i>
        <span class="screen-reader-text">email</span>
      </a>
    </li>
    

    

    


    
    <li>
      <a href="https://github.com/AyatoAshihara/my_blog" data-animate-hover="pulse" class="github" target="_blank">
        <i class="fab fa-github" title="github"></i>
        <span class="screen-reader-text">github</span>
      </a>
    </li>
    


    

    
    <li>
      <a href="/my_blog/index.xml" data-animate-hover="pulse" class="rss" target="_blank">
        <i class="fas fa-rss" title="rss"></i>
        <span class="screen-reader-text">rss</span>
      </a>
    </li>
    


  </ul></div>

      </div>

      <div class="container">
        

<div id="title-info" class="title-info">
  <div id='site-title' class='site-title'>
    
    <a href="/my_blog"> 東京の資産運用会社で働く社会人が研究に没頭するブログ </a>
    </div>
  </div>
  <button id="toggle-navigation" class="toggle-navigation">
    <i class="fas fa-bars"></i>
  </button>

  <div id="menu-primary-tracks" class="menu-primary-tracks"></div>
  <div id="menu-primary" class="menu-container menu-primary" role="navigation">
    
    <p class="site-description">院卒2年目の社会人が夜な夜な更新中。本ブログの内容は筆者が所属する組織の公式見解とは全く関係ありません。</p>
    

    <div class="menu">
      <ul id="menu-primary-items" class="menu-primary-items">
        
        
        <li id="menu-item" class='menu-item menu-item-type-post_type menu-item-object-page '>
          <a href="/my_blog/">Home</a>
          
        </li>
        
        <li id="menu-item" class='menu-item menu-item-type-post_type menu-item-object-page '>
          <a href="/my_blog/about/">About</a>
          
        </li>
        
        <li id="menu-item" class='menu-item menu-item-type-post_type menu-item-object-page '>
          <a href="/my_blog/contact/">Get in touch</a>
          
        </li>
        
      </ul>
    </div>

  </div>

      </div>
    </header>

    <div id="main" class="main" role="main">

      
  
  
    
  
  
  <div id="loop-container" class="loop-container">
    

      <div class="post type-post status-publish format-standard hentry category-standard category-travel entry full-without-featured odd excerpt-1">

        
        <div class="entry-meta">
          <span class="date">26 May</span>	<span> / </span>

          <span class="author">
            <a href="/my_blog/" title="Posts by " rel="author"></a>
          </span>


          
          <span class="category">
            <span> / </span>

            <a href="/categories/%E6%97%A5%E6%AC%A1gdp">日次GDP</a>
          </span>
          


        </div>
        <div class='entry-header'>
          <h1 class='entry-title'> 【日次GDP】GPLVMでマルチファクターモデルを構築してみた</h1>
        </div>
        <div class="entry-container">
          <div class="entry-content">
            <article>
              <p>おはこんばんにちは。
ずいぶん前にGianonne et al (2008)のマルチファクターモデルで四半期GDPの予想を行いました。
結果としては、ある程度は予測精度が出ていたものの彼らの論文ほどは満足のいくものではありませんでした。原因としてはクロスセクショナルなデータ不足が大きいと思われ、現在収集方法についてもEXCELを用いて改修中です。しかし一方で、マルチファクターモデルの改善も考えたいと思っています。前回は月次経済統計を主成分分析（実際にはカルマンフィルタ）を用いて次元削減を行い、主成分得点を説明変数としてGDPに回帰しました。今回はこの主成分分析のド発展版であるGaussian Process Latent Variable Model(GPLVM)を用いてファクターを計算し、それをGDPに回帰したいと思います。</p>
<div id="gplvm" class="section level4">
<h4>・GPLVMとは</h4>
<p>GPLVMとは、Gaussian Process Modelの一種です。以前、Gaussian Process Regressionの記事を書きました。</p>
<p>最も基本的なGaussian Process Modelは上の記事のようなモデルで、非説明変数<span class="math inline">\(Y=(y_{1},y_{2},...,y_{n})\)</span>と説明変数<span class="math inline">\(X=(\textbf{x}_{1},\textbf{x}_{2},...,\textbf{x}_{n})\)</span>があり、以下のような関係式で表される際にそのモデルを直接推定することなしに新たな説明変数<span class="math inline">\(X\)</span>の入力に対し、非説明変数<span class="math inline">\(Y\)</span>の予測値をはじき出すというものでした。</p>
<p><span class="math display">\[
\displaystyle y_{i}  = \textbf{w}^{T}\phi(\textbf{x}_{i})
\]</span></p>
<p>ここで、<span class="math inline">\(\textbf{x}_{i}\)</span>は<span class="math inline">\(i\)</span>番目の説明変数ベクトル、<span class="math inline">\(\phi(・)\)</span>は非線形関数、 <span class="math inline">\(\textbf{w}^{T}\)</span>は各入力データに対する重み係数（回帰係数）ベクトルです。非線形関数としては、<span class="math inline">\(\phi(\textbf{x}_{i}) = (x_{1,i}, x_{1,i}^{2},...,x_{1,i}x_{2,i},...)\)</span>を想定しています（<span class="math inline">\(x_{1,i}\)</span>は<span class="math inline">\(i\)</span>番目の入力データ<span class="math inline">\(\textbf{x}_{i}\)</span>の１番目の変数）。詳しくは過去記事を参照してください。</p>
<p>今回やるGPLVMは説明変数ベクトルが観測できない潜在変数（Latent Variable）であるところが特徴です。以下のスライドが非常にわかりやすいですが、GP-LVMは確率的主成分分析（PPCA）の非線形版という位置付けになっています。</p>
<p><a href="https://www.slideshare.net/antiplastics/pcagplvm:embed:cite" class="uri">https://www.slideshare.net/antiplastics/pcagplvm:embed:cite</a></p>
<p>では具体的な説明に移ります。GPLVMは主成分分析の発展版ですので、主に次元削減のために行われることを想定しています。つまり、データセットがあったとして、サンプルサイズ<span class="math inline">\(n\)</span>よりも変数の次元<span class="math inline">\(p\)</span>が大きいような場合を想定しています。</p>
</div>
<div id="primitivegp-lvmhttppapers.nips.ccpaper2540-gaussian-process-latent-variable-models-for-visualisation-of-high-dimensional-data.pdf" class="section level4">
<h4>・最もPrimitiveなGP-LVM（<a href="http://papers.nips.cc/paper/2540-gaussian-process-latent-variable-models-for-visualisation-of-high-dimensional-data.pdf" class="uri">http://papers.nips.cc/paper/2540-gaussian-process-latent-variable-models-for-visualisation-of-high-dimensional-data.pdf</a> ）</h4>
<p>先述したようにGPLVMはPPCAの非線形版です。なので、GPLVMを説明するスタートはPPCAになります。観測可能な<span class="math inline">\(D\)</span>次元データセットを<span class="math inline">\(\{\textbf{y}_{n}\}_{n=1}^{N}\)</span>とします。そして、潜在変数を<span class="math inline">\(\textbf{x}_{n}\)</span>とおきます。今、データセットと潜在変数の間には以下のような関係があるとします。</p>
<p><span class="math display">\[
\textbf{y}_{n} = \textbf{W}\textbf{x}_{n} + \epsilon_{n}
\]</span></p>
<p>ここで、<span class="math inline">\(\textbf{W}\)</span>はウェイト行列、<span class="math inline">\(\epsilon_{n}\)</span>はかく乱項で<span class="math inline">\(N(0,\beta^{-1}\textbf{I})\)</span>に従います（被説明変数が多次元になることに注意）。また、<span class="math inline">\(\textbf{x}_{n}\)</span>は<span class="math inline">\(N(0,\textbf{I})\)</span>に従います。このとき、<span class="math inline">\(\textbf{y}_{n}\)</span>の尤度を<span class="math inline">\(\textbf{x}_{n}\)</span>を周辺化することで表現すると、</p>
<p><span class="math display">\[
\begin{eqnarray*}
\displaystyle p(\textbf{y}_{n}|\textbf{W},\beta) &amp;=&amp; \int p(\textbf{y}_{n}|\textbf{x}_{n},\textbf{W},\beta)N(0,\textbf{I})d\textbf{x}_{n} \\
\displaystyle &amp;=&amp; \int N(\textbf{W}\textbf{x}_{n},\beta^{-1}\textbf{I})N(0,\textbf{I})d\textbf{x}_{n} \\
&amp;=&amp; N(0,\textbf{W}\textbf{W}^{T} + \beta^{-1}\textbf{I}) \\
\displaystyle &amp;=&amp; \frac{1}{(2\pi)^{DN/2}|\textbf{W}\textbf{W}^{T} + \beta^{-1}\textbf{I}|^{N/2}}\exp(\frac{1}{2}\textbf{tr}( (\textbf{W}\textbf{W}^{T} + \beta^{-1}\textbf{I})^{-1}\textbf{YY}^{T}))
\end{eqnarray*}
\]</span></p>
<p>となります。ここで、<span class="math inline">\(p(\textbf{y}_{n}|\textbf{x}_{n},\textbf{W},\beta)=N(\textbf{W}\textbf{x}_{n},\beta^{-1}\textbf{I})\)</span>です。平均と分散は以下から求めました。</p>
<p><span class="math display">\[
\begin{eqnarray*}
E(\textbf{y}_{n}|\textbf{W},\beta) &amp;=&amp; E(\textbf{W}\textbf{x}_{n} + \epsilon_{n}) \\
&amp;=&amp; E(\textbf{W}\textbf{x}_{n}) + E(\epsilon_{n}) \\
&amp;=&amp; \textbf{W}E(\textbf{x}_{n}) + E(\epsilon_{n}) = 0
\end{eqnarray*}
\]</span></p>
<p><span class="math display">\[
\begin{eqnarray*}
E[(\textbf{y}_{n}|\textbf{W},\beta)(\textbf{y}_{n}|\textbf{W},\beta)^{T}] &amp;=&amp; E[ (\textbf{W}\textbf{x}_{n} + \epsilon_{n} - 0)(\textbf{W}\textbf{x}_{n} + \epsilon_{n} - 0)^{T} ] \\
&amp;=&amp; E[ (\textbf{W}\textbf{x}_{n} + \epsilon_{n})(\textbf{W}\textbf{x}_{n} + \epsilon_{n})^{T} ] \\
&amp;=&amp; E[ \textbf{W}\textbf{x}_{n}(\textbf{W}\textbf{x}_{n})^{T} + \textbf{W}\textbf{x}_{n}\epsilon_{n}^{T} + \epsilon_{n}\textbf{W}\textbf{x}_{n}^{T} + \epsilon_{n}\epsilon_{n}^{T} ] \\
&amp;=&amp; E[ \textbf{W}\textbf{x}_{n}(\textbf{W}\textbf{x}_{n})^{T} + \epsilon_{n}\epsilon_{n}^{T} ] \\
&amp;=&amp; E[ \textbf{W}\textbf{x}_{n}\textbf{x}_{n}^{T}\textbf{W}^{T} + \epsilon_{n}\epsilon_{n}^{T} ] \\
&amp;=&amp; E[ \textbf{W}\textbf{x}_{n}\textbf{x}_{n}^{T}\textbf{W}^{T}] + E[\epsilon_{n}\epsilon_{n}^{T} ] \\
&amp;=&amp; \textbf{W}\textbf{W}^{T} + \beta^{-1}\textbf{I}
\end{eqnarray*}
\]</span></p>
<p><span class="math inline">\(\textbf{W}\)</span>を求めるためには<span class="math inline">\(\textbf{y}_{n}\)</span>がi.i.d.と仮定し、以下のようなデータセット全体の尤度を最大化すれば良いことになります。</p>
<p><span class="math display">\[
\displaystyle p(\textbf{Y}|\textbf{W},\beta) = \prod_{n=1}^{N}p(\textbf{y}_{n}|\textbf{W},\beta)
\]</span></p>
<p>ここで、<span class="math inline">\(\textbf{Y}\)</span>は<span class="math inline">\(N×D\)</span>の計画行列です。このように、PPCAでは<span class="math inline">\(\textbf{x}_{n}\)</span>を周辺化し、<span class="math inline">\(\textbf{W}\)</span>を最適化します。逆に、Lawrence(2004)では<span class="math inline">\(\textbf{W}\)</span>を周辺化し、<span class="math inline">\(\textbf{x}_{n}\)</span>します（理由は後述）。<span class="math inline">\(\textbf{W}\)</span>を周辺化するために、<span class="math inline">\(\textbf{W}\)</span>に事前分布を与えましょう。</p>
<p><span class="math display">\[
\displaystyle p(\textbf{W}) = \prod_{i=1}^{D}N(\textbf{w}_{i}|0,\alpha^{-1}\textbf{I})
\]</span></p>
<p>ここで、<span class="math inline">\(\textbf{w}_{i}\)</span>はウェイト行列<span class="math inline">\(\textbf{W}\)</span>の<span class="math inline">\(i\)</span>番目の列です。では、<span class="math inline">\(\textbf{W}\)</span>を周辺化して<span class="math inline">\(\textbf{Y}\)</span>の尤度関数を導出してみます。やり方はさっきとほぼ同じなので省略します。</p>
<p><span class="math display">\[
\displaystyle p(\textbf{Y}|\textbf{X},\beta) = \frac{1}{(2\pi)^{DN/2}|K|^{D/2}}\exp(\frac{1}{2}\textbf{tr}(\textbf{K}^{-1}\textbf{YY}^{T}))
\]</span></p>
<p>ここで、<span class="math inline">\(\textbf{K}=\alpha^2\textbf{X}\textbf{X}^{T} + \beta^{-1}\textbf{I}\)</span>は<span class="math inline">\(p(\textbf{Y}|\textbf{X},\beta)\)</span>の分散共分散行列で、<span class="math inline">\(\textbf{X}=(\textbf{x}_{1},\textbf{x}_{2},...,\textbf{x}_{N})^{T}\)</span>は入力ベクトルです。対数尤度は</p>
<p><span class="math display">\[
\displaystyle L = - \frac{DN}{2}\ln{2\pi} - \frac{1}{2}\ln{|\textbf{K}|} - \frac{1}{2}\textbf{tr}(\textbf{K}^{-1}\textbf{YY}^{T})
\]</span></p>
<p>周辺化のおかげでウェイト<span class="math inline">\(\textbf{W}\)</span>が消えたのでこれを<span class="math inline">\(X\)</span>で微分してみましょう。</p>
<p><span class="math display">\[
\displaystyle\frac{\partial L}{ \partial \textbf{X}} = \alpha^2 \textbf{K}^{-1}\textbf{Y}\textbf{Y}^{T}\textbf{K}^{-1}\textbf{X} - \alpha^2 D\textbf{K}^{-1}\textbf{X}
\]</span></p>
<p>ここから、</p>
<p><span class="math display">\[
\displaystyle \frac{1}{D}\textbf{Y}\textbf{Y}^{T}\textbf{K}^{-1}\textbf{X} = \textbf{X}
\]</span></p>
<p>ここで、特異値分解を用いると</p>
<p><span class="math display">\[
\textbf{X} = \textbf{ULV}^{T}
\]</span></p>
<p>となります。<span class="math inline">\(\textbf{U} = (\textbf{u}_{1},\textbf{u}_{2},...,\textbf{u}_{q})\)</span>は<span class="math inline">\(N×q\)</span>直交行列、<span class="math inline">\(\textbf{L} = diag(l_{1},l_{2},..., l_{q})\)</span>は<span class="math inline">\(q×q\)</span>の特異値を対角成分に並べた行列、<span class="math inline">\(\textbf{V}\)</span>は<span class="math inline">\(q×q\)</span>直交行列です。これを先ほどの式に代入すると、</p>
<p><span class="math display">\[
\begin{eqnarray*}
\textbf{K}^{-1}\textbf{X} &amp;=&amp; (\alpha^2\textbf{X}\textbf{X}^{T} + \beta^{-1}\textbf{I})^{-1}\textbf{X} \\
&amp;=&amp; \textbf{X}(\alpha^2\textbf{X}^{T}\textbf{X} + \beta^{-1}\textbf{I})^{-1} \\
&amp;=&amp; \textbf{ULV}^{T}(\alpha^2\textbf{VLU}^{T}\textbf{ULV}^{T} + \beta^{-1}\textbf{I})^{-1} \\
&amp;=&amp; \textbf{ULV}^{T}\textbf{V}(\alpha^2\textbf{LU}^{T}\textbf{UL} + \beta^{-1}\textbf{I}^{-1})\textbf{V}^{T} \\
&amp;=&amp; \textbf{UL}(\alpha^2\textbf{L}^{2} + \beta^{-1}\textbf{I})^{-1}\textbf{V}^{T}
\end{eqnarray*}
\]</span></p>
<p>なので、</p>
<p><span class="math display">\[
\begin{eqnarray*}
\displaystyle \frac{1}{D}\textbf{Y}\textbf{Y}^{T}\textbf{UL}(\alpha^2\textbf{L}^{2} + \beta^{-1}\textbf{I})^{-1})\textbf{V}^{T} &amp;=&amp; \textbf{ULV}^{T}\\
\displaystyle \textbf{Y}\textbf{Y}^{T}\textbf{UL} &amp;=&amp; D\textbf{U}(\alpha^2\textbf{L}^{2} + \beta^{-1}\textbf{I})^{-1}\textbf{L} \\
\end{eqnarray*}
\]</span></p>
<p>となります。<span class="math inline">\(l_{j}\)</span>が0でなければ、<span class="math inline">\(\textbf{Y}\textbf{Y}^{T}\textbf{u}_{j} = D(\alpha^2 l_{j}^{2} + \beta^{-1})\textbf{u}_{j}\)</span>となり、<span class="math inline">\(\textbf{U}\)</span>のそれぞれの列は<span class="math inline">\(\textbf{Y}\textbf{Y}^{T}\)</span>の固有ベクトルであり、対応する固有値<span class="math inline">\(\lambda_{j}\)</span>は<span class="math inline">\(D(\alpha^2 l_{j}^{2} + \beta^{-1})\)</span>となります。つまり、未知であった<span class="math inline">\(X=ULV\)</span>が実は<span class="math inline">\(\textbf{Y}\textbf{Y}^{T}\)</span>の固有値問題から求めることが出来るというわけです。<span class="math inline">\(l_{j}\)</span>は上式を利用して、</p>
<p><span class="math display">\[
\displaystyle l_{j} = (\frac{\lambda_{j}}{D\alpha^2} - \frac{1}{\beta\alpha^2})^{1/2}
\]</span></p>
<p>と<span class="math inline">\(\textbf{Y}\textbf{Y}^{T}\)</span>の固有値<span class="math inline">\(\lambda_{j}\)</span>とパラメータから求められることがわかります。よって、<span class="math inline">\(X=ULV\)</span>は</p>
<p><span class="math display">\[
\textbf{X} = \textbf{U}_{q}\textbf{L}\textbf{V}^{T}
\]</span></p>
<p>となります。ここで、<span class="math inline">\(\textbf{U}_{q}\)</span>は<span class="math inline">\(\textbf{Y}\textbf{Y}^{T}\)</span>の固有ベクトルを<span class="math inline">\(q\)</span>個取り出したものです。<span class="math inline">\(\beta\)</span>が限りなく大きければ（=観測誤差が限りなく小さければ）通常のPCAと一致します。</p>
<p>以上がPPCAです。GPLVMはPPCAで確率モデルとして想定していた以下のモデルを拡張します。</p>
<p><span class="math display">\[
\textbf{y}_{n} = \textbf{W}^{T}\textbf{x}_{n} + \epsilon_{n}
\]</span></p>
<p>具体的には、通常のガウス過程と同様、</p>
<p><span class="math display">\[
\displaystyle y_{i}  = \textbf{W}^{T}\phi(\textbf{x}_{i})+ \epsilon_{n}
\]</span></p>
<p>という風に基底関数<span class="math inline">\(\phi(\textbf{x}_{i})\)</span>をかませて拡張します。<span class="math inline">\(\phi(・)\)</span>は平均<span class="math inline">\(\textbf{0}\)</span>、分散共分散行列<span class="math inline">\(\textbf{K}_{\textbf{x}}\)</span>のガウス過程と仮定します。分散共分散行列<span class="math inline">\(\textbf{K}_{\textbf{x}}\)</span>は</p>
<p><span class="math display">\[
$\textbf{K}_{\textbf{x}}$ = \alpha^2\phi(\textbf{x})\phi(\textbf{x})^T
\]</span></p>
<p>であり、入力ベクトル<span class="math inline">\(\textbf{X}\)</span>を<span class="math inline">\(\phi(\textbf{・})\)</span>で非線形変換した特徴量<span class="math inline">\(\phi(\textbf{x})\)</span>が近いほど、出力値<span class="math inline">\(\textbf{Y}\)</span>も近くなりやすいという性質があることになります。GPLVMではこの性質を逆に利用しています。つまり、出力値<span class="math inline">\(Y_i\)</span>と<span class="math inline">\(Y_j\)</span>が近い→<span class="math inline">\(\phi(\textbf{x}_i)\)</span>と<span class="math inline">\(\phi(\textbf{x}_j)\)</span>が近い（内積が大きい）→<span class="math inline">\(\textbf{K}_{x,ij}\)</span>が大きい→観測不可能なデータ<span class="math inline">\(X_{i}\)</span>と<span class="math inline">\(X_{j}\)</span>は近い値（or同じようなパターン）をとる。
この議論からもわかるように、<span class="math inline">\(\textbf{K}_{\textbf{x}}\)</span>は入力ベクトル<span class="math inline">\(\textbf{X}\)</span>それぞれの距離を表したものになります。分散共分散行列の計算には入力ベクトル<span class="math inline">\(\textbf{X}\)</span>を基底関数<span class="math inline">\(\phi(\textbf{・})\)</span>で非線形変換した後、内積を求めるといったことをする必要はなく、カーネル関数を計算するのみでOKです。今回は王道中の王道RBFカーネルを使用していますので、これを例説明します。</p>
<p>RBFカーネル（スカラーに対する）
<span class="math display">\[
\theta_{1}\exp(-\frac{1}{\theta_{2}}(x-x^T)^2)
\]</span></p>
<p>このRBFカーネルは以下の基底関数と対応しています。</p>
<p><span class="math display">\[
\phi(x)_h = \tau\exp(-\frac{1}{r}(x-h)^2)
\]</span></p>
<p>例えば、この基底関数で入力<span class="math inline">\(x\)</span>を変換したものを<span class="math inline">\(2H^2+1\)</span>個並べた関数を</p>
<p><span class="math display">\[
\phi(x) = (\phi(x)_{-H^2}, ..., \phi(x)_{0},...,\phi(x)_{H^2})
\]</span></p>
<p>入力<span class="math inline">\(x\)</span>の特徴量だとすると<span class="math inline">\(x&#39;\)</span>との共分散<span class="math inline">\(K_{x}(x,x&#39;)\)</span>は内積の和なので</p>
<p><span class="math display">\[
K_{x}(x,x&#39;) = \sum_{h=-H^2}^{H^2}\phi_{h}(x)\phi_{h}(x&#39;)
\]</span></p>
<p>となります。ここで、<span class="math inline">\(H \to \infty\)</span>とし、グリッドを極限まで細かくしてみます。</p>
<p><span class="math display">\[
\begin{eqnarray*}
K_{x}(x,x&#39;) &amp;=&amp; \lim_{H \to \infty}\sum_{h=-H^2}^{H^2}\phi_{h}(x)\phi_{h}(x&#39;) \\
&amp;\to&amp;\int_{-\infty}^{\infty}\tau\exp(-\frac{1}{r}(x-h)^2)\tau\exp(-\frac{1}{r}(x&#39;-h)^2)dh \\
&amp;=&amp; \tau^2 \int_{-\infty}^{\infty}\exp(-\frac{1}{r}\{(x-h)^2+(x&#39;-h)^2\})dh \\
&amp;=&amp; \tau^2 \int_{-\infty}^{\infty}\exp(-\frac{1}{r}\{2(h-\frac{x+x&#39;}{2})^2+\frac{1}{2}(x-x&#39;)^2\})dh \\
\end{eqnarray*}
\]</span></p>
<p>となります。<span class="math inline">\(h\)</span>に関係の内部分を積分の外に出します。</p>
<p><span class="math display">\[
\begin{eqnarray*}
&amp;=&amp; \tau^2 \int_{-\infty}^{\infty}\exp(-\frac{2}{r}(h-\frac{x+x&#39;}{2})^2)dh\exp(-\frac{1}{2r}(x-x&#39;)^2) \\
\end{eqnarray*}
\]</span></p>
<p>残った積分を見ると、正規分布の正規化手数と等しいことがわかります。</p>
<p><span class="math display">\[
\begin{eqnarray*}
\int_{-\infty}^{\infty}\exp(-\frac{1}{2\sigma}(h-\frac{x+x&#39;}{2})^2)dh &amp;=&amp;  \int_{-\infty}^{\infty}\exp(-\frac{2}{r}(h-\frac{x+x&#39;}{2})^2)dh\\
\sigma &amp;=&amp; \frac{r}{4}
\end{eqnarray*}
\]</span></p>
<p>となるので、ガウス積分の公式を用いて</p>
<p><span class="math display">\[
\begin{eqnarray*}
&amp;=&amp; \tau^2 \sqrt{\frac{\pi r}{2}}\exp(-\frac{1}{2r}(x-x&#39;)^2)　\\
&amp;=&amp; \theta_{1}\exp(-\frac{1}{\theta_{2}}(x-x^T)^2)
\end{eqnarray*}
\]</span></p>
<p>となり、RBFカーネルと等しくなることがわかります。よって、RBFカーネルで計算した共分散は上述した基底関数で入力<span class="math inline">\(x\)</span>を無限次元へ拡張した特徴量ベクトルの内積から計算した共分散と同値になることがわかります。つまり、入力<span class="math inline">\(x\)</span>と<span class="math inline">\(x&#39;\)</span>のスカラーの計算のみで<span class="math inline">\(K_{x}(x,x&#39;)\)</span>ができてしまうという夢のような計算効率化が可能になるわけです。無限次元特徴量ベクトルの回帰問題なんて普通計算できませんからね。。。カーネル関数は偉大です。
前の記事にも載せましたが、RBFカーネルで分散共分散行列を計算したガウス過程のサンプルパスは以下通りです（<span class="math inline">\(\theta_1=1,\theta_2=0.5\)</span>）。</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Define Kernel function</span>
Kernel_Mat &lt;-<span class="st"> </span><span class="cf">function</span>(X,sigma,beta){
  N &lt;-<span class="st"> </span><span class="kw">NROW</span>(X)
  K &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>,N,N)
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
      <span class="cf">if</span>(i<span class="op">==</span>k) kdelta =<span class="st"> </span><span class="dv">1</span> <span class="cf">else</span> kdelta =<span class="st"> </span><span class="dv">0</span>
      K[i,k] &lt;-<span class="st"> </span>K[k,i] &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span><span class="kw">t</span>(X[i,]<span class="op">-</span>X[k,])<span class="op">%*%</span>(X[i,]<span class="op">-</span>X[k,])<span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>sigma<span class="op">^</span><span class="dv">2</span>)) <span class="op">+</span><span class="st"> </span>beta<span class="op">^</span>{<span class="op">-</span><span class="dv">1</span>}<span class="op">*</span>kdelta
    }
  }
  <span class="kw">return</span>(K)
}

N &lt;-<span class="st"> </span><span class="dv">10</span> <span class="co"># max value of X</span>
M &lt;-<span class="st"> </span><span class="dv">1000</span> <span class="co"># sample size</span>
X &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">seq</span>(<span class="dv">1</span>,N,<span class="dt">length=</span>M),M,<span class="dv">1</span>) <span class="co"># create X</span>
testK &lt;-<span class="st"> </span><span class="kw">Kernel_Mat</span>(X,<span class="fl">0.5</span>,<span class="fl">1e+18</span>) <span class="co"># calc kernel matrix</span>

<span class="kw">library</span>(MASS)

P &lt;-<span class="st"> </span><span class="dv">6</span> <span class="co"># num of sample path</span>
Y &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>,M,P) <span class="co"># define Y</span>

<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>P){
  Y[,i] &lt;-<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dt">n=</span><span class="dv">1</span>,<span class="kw">rep</span>(<span class="dv">0</span>,M),testK) <span class="co"># sample Y</span>
}

<span class="co"># Plot</span>
<span class="kw">matplot</span>(<span class="dt">x=</span>X,<span class="dt">y=</span>Y,<span class="dt">type =</span> <span class="st">&quot;l&quot;</span>,<span class="dt">lwd =</span> <span class="dv">2</span>)</code></pre>
<p><img src="/my_blog/post/post9_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
</div>
<div id="r" class="section level3">
<h3>Rでの実装</h3>
<p>GPLVMをRで実装します。</p>
<pre class="sourceCode r"><code class="sourceCode r">ESTIMATE_GPLVM &lt;-<span class="st"> </span><span class="cf">function</span>(Y,P,sigma){
  <span class="co"># 1. Set initial value</span>
  Y &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(Y)
  eigenvector &lt;-<span class="st"> </span><span class="kw">eigen</span>(<span class="kw">cov</span>(Y))<span class="op">$</span>vectors
  X &lt;-<span class="st"> </span>Y<span class="op">%*%</span>eigenvector[,<span class="dv">1</span><span class="op">:</span>P] <span class="co"># initial value</span>
  N &lt;-<span class="st"> </span><span class="kw">NROW</span>(Y) <span class="co"># Sample Size</span>
  D &lt;-<span class="st"> </span><span class="kw">NCOL</span>(Y) <span class="co"># Dimention of dataset</span>
  X0 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">as.vector</span>(X))
  sigma &lt;-<span class="st"> </span><span class="kw">var</span>(<span class="kw">matrix</span>(Y,<span class="kw">dim</span>(Y)[<span class="dv">1</span>]<span class="op">*</span><span class="kw">dim</span>(Y)[<span class="dv">2</span>],<span class="dv">1</span>))
  
  <span class="co"># 2. Define log likelyhood function</span>
  loglik &lt;-<span class="st"> </span><span class="cf">function</span>(X0,Y,N,P,D,beta,sigma){
    X &lt;-<span class="st"> </span><span class="kw">matrix</span>(X0,N,P)
    K &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>,N,N)
    scale &lt;-<span class="st"> </span><span class="kw">diag</span>(<span class="kw">sqrt</span>(<span class="dv">3</span><span class="op">/</span>((<span class="kw">apply</span>(X, <span class="dv">2</span>, max) <span class="op">-</span><span class="kw">apply</span>(X, <span class="dv">2</span>, min))<span class="op">^</span><span class="dv">2</span>)))
    X &lt;-<span class="st"> </span>X<span class="op">%*%</span>scale
    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
      <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
         <span class="cf">if</span>(i<span class="op">==</span>k) kdelta =<span class="st"> </span><span class="dv">1</span> <span class="cf">else</span> kdelta =<span class="st"> </span><span class="dv">0</span>
         K[i,k] &lt;-<span class="st"> </span>K[k,i] &lt;-<span class="st"> </span>sigma<span class="op">*</span><span class="kw">exp</span>(<span class="op">-</span><span class="kw">t</span>(X[i,]<span class="op">-</span>X[k,])<span class="op">%*%</span>(X[i,]<span class="op">-</span>X[k,])<span class="op">*</span><span class="fl">0.5</span>) <span class="op">+</span><span class="st"> </span>beta<span class="op">^</span>(<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>kdelta <span class="op">+</span><span class="st"> </span>beta<span class="op">^</span>(<span class="op">-</span><span class="dv">1</span>)
       }
     }
 
    L &lt;-<span class="st"> </span><span class="op">-</span><span class="st"> </span>D<span class="op">*</span>N<span class="op">/</span><span class="dv">2</span><span class="op">*</span><span class="kw">log</span>(<span class="dv">2</span><span class="op">*</span>pi) <span class="op">-</span><span class="st"> </span>D<span class="op">/</span><span class="dv">2</span><span class="op">*</span><span class="kw">log</span>(<span class="kw">det</span>(K)) <span class="op">-</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span><span class="op">*</span><span class="kw">sum</span>(<span class="kw">diag</span>(<span class="kw">ginv</span>(K)<span class="op">%*%</span>Y<span class="op">%*%</span><span class="kw">t</span>(Y))) <span class="co">#loglikelihood</span>

    <span class="kw">return</span>(L)
  }
  
  dloglik &lt;-<span class="st"> </span><span class="cf">function</span>(X0,P,D,N,Y,beta,sigma){
    X &lt;-<span class="st"> </span><span class="kw">matrix</span>(X0,N,P)
    K &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>,N,N)
    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
      <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
        <span class="cf">if</span>(i<span class="op">==</span>k) kdelta =<span class="st"> </span><span class="dv">1</span> <span class="cf">else</span> kdelta =<span class="st"> </span><span class="dv">0</span>
        K[i,k] &lt;-<span class="st"> </span>K[k,i] &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span><span class="kw">t</span>(X[i,]<span class="op">-</span>X[k,])<span class="op">%*%</span>(X[i,]<span class="op">-</span>X[k,])<span class="op">*</span><span class="fl">0.5</span>) <span class="op">+</span><span class="st"> </span>beta<span class="op">^</span>(<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>kdelta <span class="op">+</span><span class="st"> </span>beta<span class="op">^</span>(<span class="op">-</span><span class="dv">1</span>)
      }
    }
    invK &lt;-<span class="st"> </span><span class="kw">ginv</span>(K)
    dLdK &lt;-<span class="st"> </span>invK<span class="op">%*%</span>Y<span class="op">%*%</span><span class="kw">t</span>(Y)<span class="op">%*%</span>invK <span class="op">-</span><span class="st"> </span>D<span class="op">*</span>invK
    dLdx &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>,N,P)
    
    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>P){
      <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N){
        dKdx &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>,N,N)
        <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N){
          dKdx[i,k] &lt;-<span class="st"> </span>dKdx[k,i] &lt;-<span class="st"> </span><span class="op">-</span><span class="kw">exp</span>(<span class="op">-</span>(<span class="kw">t</span>(X[i,]<span class="op">-</span>X[k,])<span class="op">%*%</span>(X[i,]<span class="op">-</span>X[k,]))<span class="op">*</span><span class="fl">0.5</span>)<span class="op">*</span>((X[i,j]<span class="op">-</span>X[k,j])<span class="op">*</span><span class="fl">0.5</span>)
        }
        dLdx[i,j] &lt;-<span class="st"> </span><span class="kw">sum</span>(dLdK<span class="op">*</span>dKdx)
      }
    }
    
    <span class="kw">return</span>(dLdx)
  }
  
  <span class="co"># 3. Optimization</span>
  res &lt;-<span class="st"> </span><span class="kw">optim</span>(X0, loglik, dloglik, <span class="dt">Y =</span> Y, <span class="dt">N=</span>N, <span class="dt">P=</span>P, <span class="dt">D=</span>D, <span class="dt">beta =</span> <span class="kw">exp</span>(<span class="dv">2</span>), <span class="dt">sigma =</span> sigma,
               <span class="dt">method =</span> <span class="st">&quot;BFGS&quot;</span>, <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">fnscale =</span> <span class="dv">-1</span>,<span class="dt">trace=</span><span class="dv">1000</span>,<span class="dt">maxit=</span><span class="dv">10000</span>))
  output &lt;-<span class="st"> </span><span class="kw">matrix</span>(res<span class="op">$</span>par,N,P)
  result &lt;-<span class="st"> </span><span class="kw">list</span>(output,res,P)
  <span class="kw">names</span>(result) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;output&quot;</span>,<span class="st">&quot;res&quot;</span>,<span class="st">&quot;P&quot;</span>)
  <span class="kw">return</span>(result)
}

GPLVM_SELECT &lt;-<span class="st"> </span><span class="cf">function</span>(Y){
  D &lt;-<span class="st"> </span><span class="kw">NCOL</span>(Y)
  <span class="kw">library</span>(stringr)
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>D){
    <span class="cf">if</span> (i <span class="op">==</span><span class="st"> </span><span class="dv">1</span>){
      result &lt;-<span class="st"> </span><span class="kw">ESTIMATE_GPLVM</span>(Y,i)
      P &lt;-<span class="st"> </span><span class="dv">2</span>
      <span class="kw">print</span>(<span class="kw">str_c</span>(<span class="st">&quot;STEP&quot;</span>, i, <span class="st">&quot; loglikelihood &quot;</span>, <span class="kw">as.numeric</span>(result<span class="op">$</span>res<span class="op">$</span>value)))
    }<span class="cf">else</span>{
      temp &lt;-<span class="st"> </span><span class="kw">ESTIMATE_GPLVM</span>(Y,i)
      <span class="kw">print</span>(<span class="kw">str_c</span>(<span class="st">&quot;STEP&quot;</span>, i, <span class="st">&quot; loglikelihood &quot;</span>, <span class="kw">as.numeric</span>(temp<span class="op">$</span>res<span class="op">$</span>value)))
      <span class="cf">if</span> (result<span class="op">$</span>res<span class="op">$</span>value <span class="op">&lt;</span><span class="st"> </span>temp<span class="op">$</span>res<span class="op">$</span>value){
        result &lt;-<span class="st"> </span>temp
        P &lt;-<span class="st"> </span>i
      }
    }
  }
  <span class="kw">print</span>(<span class="kw">str_c</span>(<span class="st">&quot;The optimal number of X is &quot;</span>, P))
  <span class="kw">print</span>(<span class="kw">str_c</span>(<span class="st">&quot;loglikelihood &quot;</span>, <span class="kw">as.numeric</span>(result<span class="op">$</span>res<span class="op">$</span>value)))
  <span class="kw">return</span>(result)
}

result &lt;-<span class="st"> </span><span class="kw">ESTIMATE_GPLVM</span>(<span class="kw">scale</span>(Y),<span class="dv">5</span>)</code></pre>
<pre><code>## initial  value 854.601920 
## final  value 845.174528 
## converged</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)

<span class="kw">ggplot</span>(<span class="kw">gather</span>(<span class="kw">as.data.frame</span>(result<span class="op">$</span>output),<span class="dt">key =</span> name,<span class="dt">value =</span> value),
       <span class="kw">aes</span>(<span class="dt">x=</span><span class="kw">rep</span>(dataset1<span class="op">$</span>publication,<span class="dv">5</span>),<span class="dt">y=</span>value,<span class="dt">colour=</span>name)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">size=</span><span class="dv">1</span>)</code></pre>
<p><img src="/my_blog/post/post9_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(xts)
X.xts &lt;-<span class="st"> </span><span class="kw">xts</span>(result<span class="op">$</span>output,<span class="dt">order.by =</span> dataset1<span class="op">$</span>publication)
X.q.xts &lt;-<span class="st"> </span><span class="kw">apply.quarterly</span>(X.xts,mean)
X<span class="fl">.3</span>m.xts &lt;-<span class="st"> </span>X.xts[<span class="kw">endpoints</span>(X.xts,<span class="dt">on=</span><span class="st">&quot;quarters&quot;</span>),]
<span class="kw">colnames</span>(X.xts) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;factor1&quot;</span>,<span class="st">&quot;factor2&quot;</span>,<span class="st">&quot;factor3&quot;</span>,<span class="st">&quot;factor4&quot;</span>,<span class="st">&quot;factor5&quot;</span>) 
GDP.q &lt;-<span class="st"> </span>GDP[GDP<span class="op">$</span>publication<span class="op">&gt;</span><span class="kw">index</span>(X.q.xts)[<span class="dv">1</span>] <span class="op">&amp;</span><span class="st"> </span>GDP<span class="op">$</span>publication<span class="op">&lt;</span><span class="kw">index</span>(X.q.xts)[<span class="kw">NROW</span>(X.q.xts)],]

rg &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">scale</span>(GDP.q<span class="op">$</span>GDP)<span class="op">~</span>X.q.xts[<span class="op">-</span><span class="dv">54</span>])
rg2 &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">scale</span>(GDP.q<span class="op">$</span>GDP)<span class="op">~</span>X<span class="fl">.3</span>m.xts[<span class="op">-</span><span class="dv">54</span>])

<span class="kw">summary</span>(rg)</code></pre>
<pre><code>## 
## Call:
## lm(formula = scale(GDP.q$GDP) ~ X.q.xts[-54])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.18809 -0.12301  0.04105  0.20803  0.50298 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      0.02656    0.04344   0.611  0.54387    
## X.q.xts[-54]X.1 -0.29337    0.01606 -18.261  &lt; 2e-16 ***
## X.q.xts[-54]X.2 -0.16957    0.01912  -8.871 1.32e-11 ***
## X.q.xts[-54]X.3 -0.09971    0.02906  -3.431  0.00126 ** 
## X.q.xts[-54]X.4  0.34509    0.04519   7.636 8.95e-10 ***
## X.q.xts[-54]X.5 -0.09895    0.05018  -1.972  0.05451 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3161 on 47 degrees of freedom
## Multiple R-squared:  0.9097, Adjusted R-squared:  0.9001 
## F-statistic: 94.71 on 5 and 47 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(rg2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = scale(GDP.q$GDP) ~ X.3m.xts[-54])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.28875 -0.08787  0.02944  0.19008  0.45120 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     0.003220   0.046543   0.069  0.94513    
## X.3m.xts[-54]1 -0.287163   0.016797 -17.096  &lt; 2e-16 ***
## X.3m.xts[-54]2 -0.152909   0.020457  -7.475 1.57e-09 ***
## X.3m.xts[-54]3 -0.097886   0.031014  -3.156  0.00279 ** 
## X.3m.xts[-54]4  0.310674   0.048034   6.468 5.26e-08 ***
## X.3m.xts[-54]5 -0.007124   0.051722  -0.138  0.89103    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3387 on 47 degrees of freedom
## Multiple R-squared:  0.8963, Adjusted R-squared:  0.8853 
## F-statistic: 81.24 on 5 and 47 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>決定係数が大幅に改善しました。</p>
</div>

            </article>
          </div>
          
      <div class='entry-meta-bottom'>
        

  <div class="entry-categories"><p><span>Categories</span>
    
    <a href="/my_blog/categories/%E6%97%A5%E6%AC%A1gdp" title="View all posts in 日次GDP">日次GDP</a>
  </p>
</div>



<div class="entry-tags"><p><span>Tags</span>
  
  <a href="/my_blog/tags/r" title="View all posts tagged R">R</a>
  
  <a href="/my_blog/tags/%E3%82%AC%E3%82%A6%E3%82%B9%E9%81%8E%E7%A8%8B" title="View all posts tagged ガウス過程">ガウス過程</a>
  

</p></div>	</div>

	
<div class="author-meta">

  <div class="author">
    	
      <img alt='Ayato Ashihara' src="https://www.gravatar.com/avatar/0334adec0ab8bf709927e209f83de319?s=100&d=identicon" class='avatar avatar-72 photo' height='72' width='72'>
    
    <span>
      Written by:<a href="https://ayatoashihara.github.io/my_blog/about/" title="Posts by Ayato Ashihara" rel="author">Ayato Ashihara</a> </span>
    </div>
    <div class="bio">
      
      
      <p></p>
      
      
	

<a class="facebook" target="_blank"
href="https://www.facebook.com/ASSIY">
<i class="fab fa-facebook-f"
title="facebook icon"></i>
</a>









<a class="email" target="_blank"
href="mailto:assiy119@yahoo.co.jp">
<i class="fas fa-envelope"
title="email icon"></i>
</a>







<a class="github" target="_blank"
href="https://github.com/AyatoAshihara/my_blog">
<i class="fab fa-github"
title="github icon"></i>
</a>







</div>
</div>

</div>
</div>

<section id="comments" class="comments">
  

  




</section>
</div>

 



    </div>

    <footer id="site-footer" class="site-footer" role="contentinfo">

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ["\\(","\\)"]] } });
</script>
<script async
src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>

<script>
  hljs.initHighlightingOnLoad();
</script>

	<h1>
    
    <a href=""> 東京の資産運用会社で働く社会人が研究に没頭するブログ </a>
    
	</h1>

			
			<p class="site-description">院卒2年目の社会人が夜な夜な更新中。本ブログの内容は筆者が所属する組織の公式見解とは全く関係ありません。</p>
			

		<div id="menu-footer" class="menu-container menu-footer" role="navigation">
		<div class="menu">

      <ul id="menu-footer-items" class="menu-footer-items">
        
</ul>

</div>	</div>

<ul class="social-media-icons">

        
				<li>
					<a class="facebook" target="_blank"
					   href="https://www.facebook.com/ASSIY" >
						<i class="fab fa-facebook-f" title="facebook"></i>
						<span class="screen-reader-text">facebook</span>
					</a>
				</li>
        

        


        

        

        
        <li>
        <a href="mailto:assiy119@yahoo.co.jp"  class="email">
            <i class="fas fa-envelope" title="email"></i>
            <span class="screen-reader-text">email</span>
        </a>
        </li>
        

        

        


        
        <li>
        <a href="https://github.com/AyatoAshihara/my_blog"  class="github" target="_blank">
            <i class="fab fa-github" title="github"></i>
            <span class="screen-reader-text">github</span>
        </a>
        </li>
        


        

        
        <li>
        <a href="/my_blog/index.xml" data-animate-hover="pulse" class="rss" target="_blank">
            <i class="fas fa-rss" title="rss"></i>
            <span class="screen-reader-text">rss</span>
        </a>
        </li>
        

				</ul>	<div class="design-credit">
		
		<p>&copy; 2018 Göran Svensson</p>
		
		<p>Nederburg Hugo Theme by <a href="https://appernetic.io">Appernetic</a>.</p>
		
		<p>A port of Tracks by Compete Themes.</p>
		
	</div>
</footer>

  </div>
  <script src="/my_blog/js/jquery.min.js"></script>
<script src="/my_blog/js/jquerymigrate.js"></script>
<script src="/my_blog/js/production.min.js?v=1561192814"></script>

</body>
</html>
