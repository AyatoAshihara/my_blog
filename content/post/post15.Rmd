---
title: 【日次GDP】Google Earth Engine APIで衛星画像データを取得し、景況感をナウキャスティングしてみる（第２回）
author: Ayato Ashiara
date: '2019-08-15'
slug: post15
categories:
  - 日次GDP
tags:
  - Earth Engine
  - Python
  - 時系列解析
image: img/portfolio/earthengine6.jpg
---

<!--more-->

```{r,include=FALSE}

library(reticulate)
conda_path <- "C:\\Users\\aashi\\Anaconda3"
use_condaenv(conda_path)

```

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

おはこんばんにちは。前回の記事でGoogl Earth Engineから衛星画像データを取得しました。今回はそのデータを用いて景況感のナウキャスティングをやってみます。
まず、前回取得したデータを見ましょう。

```{python, include=FALSE}
import pandas as pd

nightjp_csv = pd.read_csv('C:/Users/aashi/Desktop/my_blog/public/dataset/jpnightlight.csv')

```

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import os

os.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = 'C:/Users/aashi/Anaconda3/Library/plugins/platforms'

plt.style.use('ggplot')
plt.xkcd()

nightjp = nightjp_csv.assign(date=lambda nightjp_csv:pd.to_datetime(nightjp_csv['system:index']))
nightjp.head()

plt.plot(nightjp['date'],nightjp['sum'])

```

かなり季節性があることがわかります。どうやら冬場に光量が大きくなる傾向になるようです。なので、季節調整をかけてみます。RではX-13ARIMA-SEATSをいつも使用していますが、pythonでの使い方がわからないので、statsmodels.apiのseasonal_decomposeを使います。冬場とそれ以外で挙動が異なるのでfreqは12にしてみました。

```{python}
import statsmodels.api as sm
nightjp = nightjp.set_index('date')
nightjp_sm = sm.tsa.seasonal_decompose(nightjp['sum'],freq=12,model='multiplicative')
nightjp_sm.plot()

```

季節性を除いたTrendが2016年半ばくらいから2017年下旬にかけて急激に上昇しています。おそらく、景況感とはあまり相関がなさそうな動きをしていますが、以下の記事を参考にestatからAPIで鉱工業生産指数のデータを落とし、検証してみます。

http://sinhrks.hatenablog.com/entry/2015/12/31/222207

```{python,include=FALSE}

key = '16aba745ce9d2640a24adcd96c0b39e60c181de4'

```

```{python}
import numpy as np
import japandas as jpd
import datetime 

# import IIP from estat api
dlist = jpd.DataReader("00550300", 'estat', appid=key)
tables = dlist[(dlist['統計表題名及び表番号'].str.contains('付加価値額生産')) & (dlist['提供統計名及び提供分類名'].str.contains('鉱工業生産・出荷・在庫指数'))]
data = jpd.DataReader("0003325360", 'estat', appid=key)
df = data[(data['業種別'].str.contains('1000000000 鉱工業')) & ~(data['統計項目A'].str.contains('付加生産ウエイト'))]
df = df.assign(date=lambda nightjp_csv:pd.to_datetime(df["統計項目A"], format="%Y%m"))
df.head()

# merge with seasonally adjusted nightlight data
df2 = pd.merge(nightjp_sm.trend.to_frame(),df,how='left',on='date')[['date','sum','value']]

# plot
fig = plt.figure()
ax1 = fig.add_subplot(1, 1, 1)
ax1.plot(df2['date'], df2['sum'],'C0',label='nightlight')
ax2 = ax1.twinx()
ax2.plot(df2['date'], df2['value'],'C1',label='IIP')
h1, l1 = ax1.get_legend_handles_labels()
h2, l2 = ax2.get_legend_handles_labels()
ax1.legend(h1+h2, l1+l2, loc='lower right')
plt.show()

```

予想に反し、かなり良い傾向を掴めていますね。季節調整を少し雑にやっているので、必要な情報もノイズとしてスクリーニングされた感がありますが、季節調整を真面目にやればかなり近い数値が出てくる気もします。ということで、X-13ARIMA-SEATSのpythonでの使い方をググりました。なんとstatsmodelsで動かせるようです。

```{python}

import statsmodels as sms

# x13 
x13results = sms.tsa.x13.x13_arima_analysis(endog = nightjp['sum'])
x13results.plot()

# merge with seasonally adjusted nightlight data
df3 = pd.merge(x13results.seasadj.to_frame(),df,how='left',on='date')[['date','seasadj','value']]

# plot
fig = plt.figure()
ax1 = fig.add_subplot(1, 1, 1)
ax1.plot(df3['date'], df3['seasadj'],'C0',label='nightlight')
ax2 = ax1.twinx()
ax2.plot(df3['date'], df3['value'],'C1',label='IIP')
h1, l1 = ax1.get_legend_handles_labels()
h2, l2 = ax2.get_legend_handles_labels()
ax1.legend(h1+h2, l1+l2, loc='lower right')
plt.show()

```

そこそこ説明力高め。これは個人的には大発見です。鉱工業生産指数はGDPとの相関が高く、月次統計でもあります。ただ、生産動向統計から作成されると言うこともあり、データが公表されるタイミングは速報値が出るのが翌月末です。一方、衛星データであれば月初から推計値を計算することが可能です。まさにナウキャスティングですね。欲を言えば、日次でデータが取れれば最高なんですけどね。多分それは有料ならできるんでしょう。。。今はこれで我慢です。（いつか使える日が来るのか？）

単回帰もやってみます。

```{python}

from sklearn import linear_model
clf = linear_model.LinearRegression(normalize=False)

X = df3.loc[:, ['seasadj']].as_matrix()
Y = df3['value'].as_matrix()

clf.fit(X, Y)

print(clf.coef_,clf.intercept_,clf.score(X, Y))

plt.scatter(X, Y)
 
plt.plot(X, clf.predict(X))

```

ほぼほぼ比例の関係にありますね。決定係数は0.5でした。散布図を見ると非線形の関係にあるようにも見えるのでガウス回帰でそれも試してみます。

```{python}

from sklearn.gaussian_process.kernels import RBF,WhiteKernel
from sklearn.gaussian_process import GaussianProcessRegressor as GPR

# kernel is RBF + white
kernel = 1*RBF()+WhiteKernel()

# estimate gp
gp = GPR(kernel,alpha=0)
gp.fit(X,Y)

# plot
X1 = np.linspace(600000,850000,25000)
plt.plot(X,Y,'. ')
mu,std = gp.predict(X1.reshape(-1, 1),return_std=True)
plt.plot(X1,mu,'g')
plt.fill_between(X1,mu-std,mu+std,alpha=0.2,color='g')
plt.show()

```

770000まではほぼ直線ですが、その先で外れ値に引っ張られています。本当は750000~800000のところで二次関数のようにぐっと上昇して欲しいのですが。外れ値に引っ張られないよう、正規分布でなくt分布を仮定したt過程回帰で推計します。

実際の推計をする前に、理論的な話をしておきます。そもそもt分布とは正規分布の分散を増減させるパラメータを新たに与え、それがガンマ分布に従うと仮定した分布です。t過程はガウス過程と同様、有限集合$\textbf{X}$が入力として与えられた際に、関数値ベクトル$\textbf{f}_{TP}$の分布がその多変量t分布に従うような確率分布を言います。t過程は自由度$v$、平均関数$\mu$、共分散関数を要素に持つ共分散行列$\textbf{K}$をパラメータとしています。これを$\textbf{f}_{TP}$~$T(v,\mu(\textbf{x}),\textbf{K}(\textbf{x},\textbf{x}'))$と表します。$T(v,\mu(\textbf{x}),\textbf{K}(\textbf{x},\textbf{x}'))$は先述の通り多変量t分布です。出力である確率変数$\textbf{y}$に対して、確率密度関数は以下のように与えられます（つまり多変量t分布の密度関数）。

$$
T(v,\mu,\textbf{K}) = \frac{\Gamma(\frac{v+n}{2})}{((v-2)\pi)^{n/2}\Gamma(\frac{v}{2})}\frac{1}{\sqrt(\det\textbf{K})}(1+\frac{(\textbf{y}-\mu)^T\textbf{K}^{-1}(\textbf{y}-\mu)}{v-2})^{-\frac{(v+n)}2}
$$

ここで、$\Gamma$はΓ関数です。ここで、$v\to\infty$とするとカーネルの部分が、

$$
\lim_{v\to\infty}(1+\frac{(\textbf{y}-\mu)^T\textbf{K}^{-1}(\textbf{y}-\mu)}{v-2})^{-\frac{(v+n)}2}=\exp(\frac{(\textbf{y}-\mu)^T\textbf{K}^{-1}(\textbf{y}-\mu)}{2})
$$

と正規分布に収束するので、先述の通りt分布は正規分布の一般系であることがわかります。次に、今定義したt過程を使った回帰問題について考えます。学習データ$(\textbf{x}_i,y_{i})$が手元にあるとします。t過程回帰では以下のようなモデルを考えます。

$$
y_{i} = f_{TP}(\textbf{x}_{i}) + \epsilon_{i}
$$
ここで、$f_{TP}(\textbf{x}_{i})$は基底関数による入力ベクトルの特徴量、$\epsilon_{i}-T(v,0,\sigma^2)$は観測ノイズを表しています。また、$\textbf{f}_{TP}=[f_{TP}(\textbf{x}_{1}),...,f_{TP}(\textbf{x}_{n})]^T$と定義し、t過程に従うとします。$\textbf{f}_{TP}$と$\epsilon_{i}$の分布がわかっていますから、$\textbf{x}$が既知となった後の$\textbf{y}$の分布を計算することができます。これはガウス過程の時と同じで、2つの独立なt分布のたたみ込みになるので、$y-T(v,\mu(\textbf{x}),\textbf{K}(\textbf{x},\textbf{x}')+\sigma\textbf{I})$となります。




