---
title: 【日次GDP】Google Earth Engine APIで衛星画像データを取得し、景況感をナウキャスティングしてみる（第２回）
author: Ayato Ashiara
date: '2019-08-15'
slug: post15
categories:
  - 日次GDP
tags:
  - Earth Engine
  - Python
  - 時系列解析
image: img/portfolio/earthengine6.jpg
---



<!--more-->
<p>おはこんばんにちは。前回の記事でGoogl Earth Enngineから衛星画像データを取得しました。今回はそのデータを用いて景況感のナウキャスティングをやってみます。
まず、前回取得したデータを見ましょう。</p>
<pre class="python"><code>import pandas as pd
import matplotlib.pyplot as plt
import os

os.environ[&#39;QT_QPA_PLATFORM_PLUGIN_PATH&#39;] = &#39;C:/Users/aashi/Anaconda3/Library/plugins/platforms&#39;

plt.style.use(&#39;ggplot&#39;)
plt.xkcd()</code></pre>
<pre><code>## &lt;matplotlib.pyplot.xkcd.&lt;locals&gt;.dummy_ctx object at 0x00000000295685C0&gt;</code></pre>
<pre class="python"><code>nightjp = nightjp_csv.assign(date=lambda nightjp_csv:pd.to_datetime(nightjp_csv[&#39;system:index&#39;]))
nightjp.head()</code></pre>
<pre><code>##   system:index          sum    ...     Unnamed: 4       date
## 0     2014/1/1  881512.4572    ...            NaN 2014-01-01
## 1     2014/2/1  827345.3551    ...            NaN 2014-02-01
## 2     2014/3/1  729110.4619    ...            NaN 2014-03-01
## 3     2014/4/1  612665.8866    ...            NaN 2014-04-01
## 4     2014/5/1  661434.5027    ...            NaN 2014-05-01
## 
## [5 rows x 6 columns]</code></pre>
<pre class="python"><code>plt.plot(nightjp[&#39;date&#39;],nightjp[&#39;sum&#39;])</code></pre>
<p><img src="/my_blog/post/post15_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>かなり季節性があることがわかります。どうやら冬場に光量が大きくなる傾向になるようです。なので、季節調整をかけてみます。RではX-13ARIMA-SEATSをいつも使用していますが、pythonでの使い方がわからないので、statsmodels.apiのseasonal_decomposeを使います。冬場とそれ以外で挙動が異なるのでfreqは12にしてみました。</p>
<pre class="python"><code>import statsmodels.api as sm
nightjp = nightjp.set_index(&#39;date&#39;)
nightjp_sm = sm.tsa.seasonal_decompose(nightjp[&#39;sum&#39;],freq=12,model=&#39;multiplicative&#39;)
nightjp_sm.plot()</code></pre>
<p><img src="/my_blog/post/post15_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>季節性を除いたTrendが2016年半ばくらいから2017年下旬にかけて急激に上昇しています。おそらく、景況感とはあまり相関がなさそうな動きをしていますが、以下の記事を参考にestatからAPIで鉱工業生産指数のデータを落とし、検証してみます。</p>
<p><a href="http://sinhrks.hatenablog.com/entry/2015/12/31/222207" class="uri">http://sinhrks.hatenablog.com/entry/2015/12/31/222207</a></p>
<pre class="python"><code>import numpy as np
import japandas as jpd
import datetime 

# import IIP from estat api
dlist = jpd.DataReader(&quot;00550300&quot;, &#39;estat&#39;, appid=key)
tables = dlist[(dlist[&#39;統計表題名及び表番号&#39;].str.contains(&#39;付加価値額生産&#39;)) &amp; (dlist[&#39;提供統計名及び提供分類名&#39;].str.contains(&#39;鉱工業生産・出荷・在庫指数&#39;))]
data = jpd.DataReader(&quot;0003325360&quot;, &#39;estat&#39;, appid=key)
df = data[(data[&#39;業種別&#39;].str.contains(&#39;1000000000 鉱工業&#39;)) &amp; ~(data[&#39;統計項目A&#39;].str.contains(&#39;付加生産ウエイト&#39;))]
df = df.assign(date=lambda nightjp_csv:pd.to_datetime(df[&quot;統計項目A&quot;], format=&quot;%Y%m&quot;))
df.head()

# merge with seasonally adjusted nightlight data</code></pre>
<pre><code>##      value             業種別   統計項目A       date
## 157   94.8  1000000000 鉱工業  201301 2013-01-01
## 314   96.5  1000000000 鉱工業  201302 2013-02-01
## 471   97.7  1000000000 鉱工業  201303 2013-03-01
## 628   97.7  1000000000 鉱工業  201304 2013-04-01
## 785   99.3  1000000000 鉱工業  201305 2013-05-01</code></pre>
<pre class="python"><code>df2 = pd.merge(nightjp_sm.trend.to_frame(),df,how=&#39;left&#39;,on=&#39;date&#39;)[[&#39;date&#39;,&#39;sum&#39;,&#39;value&#39;]]

# plot
fig = plt.figure()
ax1 = fig.add_subplot(1, 1, 1)
ax1.plot(df2[&#39;date&#39;], df2[&#39;sum&#39;],&#39;C0&#39;,label=&#39;nightlight&#39;)
ax2 = ax1.twinx()
ax2.plot(df2[&#39;date&#39;], df2[&#39;value&#39;],&#39;C1&#39;,label=&#39;IIP&#39;)
h1, l1 = ax1.get_legend_handles_labels()
h2, l2 = ax2.get_legend_handles_labels()
ax1.legend(h1+h2, l1+l2, loc=&#39;lower right&#39;)
plt.show()</code></pre>
<p><img src="/my_blog/post/post15_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>これはかなり良い傾向を掴めているのではないでしょうか。季節調整を少し雑にやっているので、必要な情報もノイズとしてスクリーニングされた感がありますが、季節調整を真面目にやればかなり近い数値が出てくる気もします。ということで、X-13ARIMA-SEATSのpythonでの使い方をググりました。なんとstatsmodelsで動かせるようです。</p>
<pre class="python"><code>
import statsmodels as sms

# x13 
x13results = sms.tsa.x13.x13_arima_analysis(endog = nightjp[&#39;sum&#39;])
x13results.plot()

# merge with seasonally adjusted nightlight data
df3 = pd.merge(x13results.seasadj.to_frame(),df,how=&#39;left&#39;,on=&#39;date&#39;)[[&#39;date&#39;,&#39;seasadj&#39;,&#39;value&#39;]]

# plot
fig = plt.figure()
ax1 = fig.add_subplot(1, 1, 1)
ax1.plot(df3[&#39;date&#39;], df3[&#39;seasadj&#39;],&#39;C0&#39;,label=&#39;nightlight&#39;)
ax2 = ax1.twinx()
ax2.plot(df3[&#39;date&#39;], df3[&#39;value&#39;],&#39;C1&#39;,label=&#39;IIP&#39;)
h1, l1 = ax1.get_legend_handles_labels()
h2, l2 = ax2.get_legend_handles_labels()
ax1.legend(h1+h2, l1+l2, loc=&#39;lower right&#39;)
plt.show()</code></pre>
<p><img src="/my_blog/post/post15_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>そこそこ説明量高め。単回帰もやってみます。</p>
<pre class="python"><code>
from sklearn import linear_model
clf = linear_model.LinearRegression(normalize=False)

X = df3.loc[:, [&#39;seasadj&#39;]].as_matrix()</code></pre>
<pre><code>## C:\Users\aashi\ANACON~1\python.exe:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.</code></pre>
<pre class="python"><code>Y = df3[&#39;value&#39;].as_matrix()

clf.fit(X, Y)</code></pre>
<pre><code>## LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)</code></pre>
<pre class="python"><code>print(clf.coef_)</code></pre>
<pre><code>## [2.24932518e-05]</code></pre>
<pre class="python"><code>print(clf.intercept_)</code></pre>
<pre><code>## 85.74128984259595</code></pre>
<pre class="python"><code>print(clf.score(X, Y))</code></pre>
<pre><code>## 0.4958790404129829</code></pre>
<pre class="python"><code>plt.scatter(X, Y)
 
plt.plot(X, clf.predict(X))</code></pre>
<p><img src="/my_blog/post/post15_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>ほぼほぼ比例の関係にありますね。決定係数は0.5でした。散布図を見ると非線形の関係にあるようにも見えるのでガウス回帰でそれも試してみます。</p>
<pre class="python"><code>
from sklearn.gaussian_process.kernels import RBF,WhiteKernel
from sklearn.gaussian_process import GaussianProcessRegressor as GPR

kernel = 1*RBF()+WhiteKernel() # sklearnのカーネル
gp = GPR(kernel,alpha=0) # MCMCで最適化するので、ここではoptimizer=None
gp.fit(X,Y)

# 近似の結果</code></pre>
<pre><code>## GaussianProcessRegressor(alpha=0, copy_X_train=True,
##              kernel=1**2 * RBF(length_scale=1) + WhiteKernel(noise_level=1),
##              n_restarts_optimizer=0, normalize_y=False,
##              optimizer=&#39;fmin_l_bfgs_b&#39;, random_state=None)</code></pre>
<pre class="python"><code>X1 = np.linspace(600000,850000,25000)
plt.plot(X,Y,&#39;. &#39;)
mu,std = gp.predict(X1.reshape(-1, 1),return_std=True)
plt.plot(X1,mu,&#39;g&#39;)
plt.fill_between(X1,mu-std,mu+std,alpha=0.2,color=&#39;g&#39;)
plt.show()</code></pre>
<p><img src="/my_blog/post/post15_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>外れ値に引っ張られています。本当は750000~800000のところで二次関数のようにぐっと上昇して欲しいのですが。外れ値に引っ張られないよう、正規分布でなくt分布を仮定したt過程回帰で推計します。</p>
